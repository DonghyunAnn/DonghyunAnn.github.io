---
layout: single
title: '예시'
categories: crawling
tags: ['python','beautifulsoup','selenium']
toc: true   # 우측 목차 여부
author_profile: false   # 좌측 프로필 여부
sidebar:
    nav: "docs"     # sidebar navigation
published: false
---
**[공지사항]**[블로그 안내 드립니다]('https://mmistakes.github.io/minimal-mistakes/docs/utility-classes/')
{: .notice--danger}

<div class="notice--info">
<h4>공지사항입니다</h4>
<ul>
    <li>공지사항순서1</li>
    <li>공지사항순서2</li>
    <li>공지사항순서3</li>
</ul>
</div>

[Google](https://www.google.com){: .btn .btn--info}

{% include video id="Z053Qn8LJyk" provider="youtube" %}
```python
# Ignore the warnings
import warnings
# warnings.filterwarnings('always')
warnings.filterwarnings('ignore')

# System related and data input controls
import os

# Data manipulation and visualization
import pandas as pd
pd.options.display.float_format = '{:,.2f}'.format
pd.options.display.max_rows = 10
pd.options.display.max_columns = 20
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Modeling algorithms
# General
import statsmodels.api as sm     #R의 기능을 그대로 가져온
from scipy import stats

# Model selection
from sklearn.model_selection import train_test_split

# Evaluation metrics
# for regression
from sklearn.metrics import mean_squared_log_error, mean_squared_error,  r2_score, mean_absolute_error
```

# 전처리 순서


```python
# 결측값 확인
df.describe(include='all').T
```


```python
# 날짜 데이터타입 변경 str -> datetime
if 'datetime' in df.columns:
    df['datetime'] = pd.to_datetime(df['datetime'])
    df['DateTime'] = pd.to_datetime(df['datetime'])
df.info()
```


```python
# 새롭게 만든 Detime 칼럼 인덱스화
if df.index.dtype == 'int64':
    df.set_index('DateTime', inplace=True)
df
```


```python
# 데이터의 빈도화 -> 빈 날짜를 채워줌 H,D,W,M...
## 빈 날짜를 채워줄 떄는 오름차순 내림차순에 따라 ffill bfill
df = df.asfreq('H', method='ffill')
```


```python
# Y = Trend + Seasonal + Residual <- 시계열을 분리하는 법
result = sm.tsa.seasonal_decompose(df['count'], model='additive')
## Y = Trend * Seasonal * Residual <- 곱하기로 분리할 수도 있는데, 각자 설명력에 따라 판단해서 사용 
sm.tsa.seasonal_decompose(df['count'], model='multiplicative')
```


```python
# 시각화
sm.tsa.seasonal_decompose(df['count'], model='additive').plot()
plt.show()
sm.tsa.seasonal_decompose(raw_all['count'], model='multiplicative').plot()
plt.show()
```


```python
# 분리해서 추출한 trend seasonal 앞뒤 공백값 채우고 데아터 붙이기
Y_trend = pd.DataFrame(result.trend)
Y_trend.fillna(method='ffill', inplace=True)
Y_trend.fillna(method='bfill', inplace=True)
Y_trend.columns = ['count_trend']
Y_seasonal = pd.DataFrame(result.seasonal)
Y_seasonal.fillna(method='ffill', inplace=True)
Y_seasonal.fillna(method='bfill', inplace=True)
Y_seasonal.columns = ['count_seasonal']

if 'count_trend' not in raw_all.columns:
    if 'count_seasonal' not in raw_all.columns:
        df = pd.concat([df, Y_trend, Y_seasonal], axis=1)
df
```


```python
# 이동평균 만들고 앞뒤 채워서 데이터 붙이기
Y_count_Day = df[['count']].rolling(24).mean()
Y_count_Day.fillna(method='ffill', inplace=True)
Y_count_Day.fillna(method='bfill', inplace=True)
Y_count_Day.columns = ['count_Day']
Y_count_Week = df[['count']].rolling(24*7).mean()
Y_count_Week.fillna(method='ffill', inplace=True)
Y_count_Week.fillna(method='bfill', inplace=True)
Y_count_Week.columns = ['count_Week']
if 'count_Day' not in df.columns:
    df = pd.concat([df, Y_count_Day], axis=1)
if 'count_Week' not in df.columns:
    df = pd.concat([df, Y_count_Week], axis=1)
df
```


```python
# 차분 계산 후 데이터 붙이기
# diff of Y and merging
Y_diff = df[['count']].diff()
Y_diff.fillna(method='ffill', inplace=True)
Y_diff.fillna(method='bfill', inplace=True)
Y_diff.columns = ['count_diff']
if 'count_diff' not in df.columns:
    df = pd.concat([df, Y_diff], axis=1)
df
```


```python
# 시간정보 feature 추가
df['Year'] = df.datetime.dt.year
df['Quater'] = df.datetime.dt.quarter
## 분기를 어떻게 해석할지에 따라서 칼럼을 생성할 수 있음
df['Quater_ver2'] = df['Quater'] + (df.Year - df.Year.min()) * 4
df['Month'] = df.datetime.dt.month
df['Day'] = df.datetime.dt.day
df['Hour'] = df.datetime.dt.hour
df['DayofWeek'] = df.datetime.dt.dayofweek    # 0,1,2,3,4,5,6
df
```


```python
# 시간 지연값 추가 lagged time
## shift(n): n개의 행만큼 밀기 -면 위로
df['count_lag1'] = df['count'].shift(1)
df['count_lag2'] = df['count'].shift(2)
# fill nan as some values
df['count_lag1'].fillna(method='bfill', inplace=True)
df['count_lag2'].fillna(method='bfill', inplace=True)
df
```


```python
# 더미화
if 'Quater' in df.columns:
    # 기존의 컬럼 이름 앞에 이름 지정은 prefix
    # 더미로 만든 변수 중 하나 제거 drop_first
    df = pd.concat([df, pd.get_dummies(df['Quater'], prefix='Quater_Dummy', drop_first=True)], axis=1)
    del df['Quater']
df
```


```python
# 분석에는 숫자만 들어가야하기 때문에 숫자 외의 값들을 제거 카테고리
df.loc[:, [col for col in df.columns if col != 'temp_group']].info()
```

## Code Summary


```python
### Functinalize
### Feature engineering of default
def non_feature_engineering(raw):
    # 시간축만 생성
    raw_nfe = raw.copy()
    if 'datetime' in raw_nfe.columns:
        raw_nfe['datetime'] = pd.to_datetime(raw_nfe['datetime'])
        raw_nfe['DateTime'] = pd.to_datetime(raw_nfe['datetime'])
    if raw_nfe.index.dtype == 'int64':
        raw_nfe.set_index('DateTime', inplace=True)
    # bring back
    # if raw_nfe.index.dtype != 'int64':
    #     raw_nfe.reset_index(drop=False, inplace=True)
    raw_nfe = raw_nfe.asfreq('H', method='ffill')
    return raw_nfe
# raw_rd = non_feature_engineering(raw_all)


### Feature engineering of all
# 그 이후의 모든 과정
def feature_engineering(raw):
    raw_fe = raw.copy()
    if 'datetime' in raw_fe.columns:
        raw_fe['datetime'] = pd.to_datetime(raw_fe['datetime'])
        raw_fe['DateTime'] = pd.to_datetime(raw_fe['datetime'])

    if raw_fe.index.dtype == 'int64':
        raw_fe.set_index('DateTime', inplace=True)

    raw_fe = raw_fe.asfreq('H', method='ffill')

    result = sm.tsa.seasonal_decompose(raw_fe['count'], model='additive')
    Y_trend = pd.DataFrame(result.trend)
    Y_trend.fillna(method='ffill', inplace=True)
    Y_trend.fillna(method='bfill', inplace=True)
    Y_trend.columns = ['count_trend']
    Y_seasonal = pd.DataFrame(result.seasonal)
    Y_seasonal.fillna(method='ffill', inplace=True)
    Y_seasonal.fillna(method='bfill', inplace=True)
    Y_seasonal.columns = ['count_seasonal']
    pd.concat([raw_fe, Y_trend, Y_seasonal], axis=1).isnull().sum()
    if 'count_trend' not in raw_fe.columns:
        if 'count_seasonal' not in raw_fe.columns:
            raw_fe = pd.concat([raw_fe, Y_trend, Y_seasonal], axis=1)

    Y_count_Day = raw_fe[['count']].rolling(24).mean()
    Y_count_Day.fillna(method='ffill', inplace=True)
    Y_count_Day.fillna(method='bfill', inplace=True)
    Y_count_Day.columns = ['count_Day']
    Y_count_Week = raw_fe[['count']].rolling(24*7).mean()
    Y_count_Week.fillna(method='ffill', inplace=True)
    Y_count_Week.fillna(method='bfill', inplace=True)
    Y_count_Week.columns = ['count_Week']
    if 'count_Day' not in raw_fe.columns:
        raw_fe = pd.concat([raw_fe, Y_count_Day], axis=1)
    if 'count_Week' not in raw_fe.columns:
        raw_fe = pd.concat([raw_fe, Y_count_Week], axis=1)

    Y_diff = raw_fe[['count']].diff()
    Y_diff.fillna(method='ffill', inplace=True)
    Y_diff.fillna(method='bfill', inplace=True)
    Y_diff.columns = ['count_diff']
    if 'count_diff' not in raw_fe.columns:
        raw_fe = pd.concat([raw_fe, Y_diff], axis=1)

    raw_fe['temp_group'] = pd.cut(raw_fe['temp'], 10)
    raw_fe['Year'] = raw_fe.datetime.dt.year
    raw_fe['Quater'] = raw_fe.datetime.dt.quarter
    raw_fe['Quater_ver2'] = raw_fe['Quater'] + (raw_fe.Year - raw_fe.Year.min()) * 4
    raw_fe['Month'] = raw_fe.datetime.dt.month
    raw_fe['Day'] = raw_fe.datetime.dt.day
    raw_fe['Hour'] = raw_fe.datetime.dt.hour
    raw_fe['DayofWeek'] = raw_fe.datetime.dt.dayofweek

    raw_fe['count_lag1'] = raw_fe['count'].shift(1)
    raw_fe['count_lag2'] = raw_fe['count'].shift(2)
    raw_fe['count_lag1'].fillna(method='bfill', inplace=True)
    raw_fe['count_lag2'].fillna(method='bfill', inplace=True)

    if 'Quater' in raw_fe.columns:
        if 'Quater_Dummy' not in ['_'.join(col.split('_')[:2]) for col in raw_fe.columns]:
            raw_fe = pd.concat([raw_fe, pd.get_dummies(raw_fe['Quater'], prefix='Quater_Dummy', drop_first=True)], axis=1)
            del raw_fe['Quater']
    return raw_fe
# # 실행
# raw_fe = feature_engineering(raw_all)
```


```python

```


```python

```


```python

```

# 데이터 분리


```python
# 칼럼명 불러오기
Y_colname = ['count']
X_remove = ['datetime', 'DateTime', 'temp_group', 'casual', 'registered']
X_colname = [x for x in df.columns if x not in Y_colname+X_remove]
```


```python
# 특정시간 기준으로 나누기
raw_train = df.loc[df.index < '2012-07-01',:]
raw_test = df.loc[df.index >= '2012-07-01',:]
```


```python
# 지정 칼럼 추출
Y_train = raw_train[Y_colname]
X_train = raw_train[X_colname]
Y_test = raw_test[Y_colname]
X_test = raw_test[X_colname]
print(X_train.shape, Y_train.shape)
print(X_test.shape, Y_test.shape)
```

## Code Summary


```python
### Functionalize
### Data split of cross sectional
def datasplit_cs(raw, Y_colname, X_colname, test_size, random_seed=123):
    X_train, X_test, Y_train, Y_test = train_test_split(raw[X_colname], raw[Y_colname], test_size=test_size, random_state=random_seed)
    print('X_train:', X_train.shape, 'Y_train:', Y_train.shape)
    print('X_test:', X_test.shape, 'Y_test:', Y_test.shape)
    return X_train, X_test, Y_train, Y_test
# X_train, X_test, Y_train, Y_test = datasplit_cs(raw_fe, Y_colname, X_colname, 0.2)


### Data split of time series
def datasplit_ts(raw, Y_colname, X_colname, criteria):
    raw_train = raw.loc[raw.index < criteria,:]
    raw_test = raw.loc[raw.index >= criteria,:]
    Y_train = raw_train[Y_colname]
    X_train = raw_train[X_colname]
    Y_test = raw_test[Y_colname]
    X_test = raw_test[X_colname]
    print('Train_size:', raw_train.shape, 'Test_size:', raw_test.shape)
    print('X_train:', X_train.shape, 'Y_train:', Y_train.shape)
    print('X_test:', X_test.shape, 'Y_test:', Y_test.shape)
    return X_train, X_test, Y_train, Y_test

# # 실행
# Y_colname = ['count']
# X_remove = ['datetime', 'DateTime', 'temp_group', 'casual', 'registered']
# X_colname = [x for x in raw_fe.columns if x not in Y_colname+X_remove]
# X_train, X_test, Y_train, Y_test = datasplit_ts(raw_fe, Y_colname, X_colname, '2012-07-01')
```

# 예측 (Base model)



```python
# 적합
fit_reg1 = sm.OLS(Y_train, X_train).fit()
# 결과제시
display(fit_reg1.summary())
pred_tr_reg1 = fit_reg1.predict(X_train).values
pred_te_reg1 = fit_reg1.predict(X_test).values
```

# 성능 확인


```python
# precision comparisions
pd.concat([Y_train, pd.DataFrame(pred_tr_reg1,
                                 index=Y_train.index,
                                 columns=['prediction'])],
          axis=1).plot(kind='line', figsize=(20,6),
                                                                                                               xlim=(Y_train.index.min(),Y_train.index.max()),
                                                                                                               linewidth=0.5, fontsize=20)
plt.title('Time Series of Target', fontsize=20)
plt.xlabel('Index', fontsize=15)
plt.ylabel('Target Value', fontsize=15)
plt.show()
```

# 잔차분석


```python
# 인덱스 만들기
Resid_tr_reg1['RowNum'] = Resid_tr_reg1.reset_index().index
```


```python
# 회귀선 시각화
sns.set(palette="muted", color_codes=True, font_scale=2)
# fit_reg = 회귀선 추가, line_kws = 색 입히기
sns.lmplot(data=Resid_tr_reg1.iloc[1:], x='RowNum', y='Error',
           fit_reg=True, line_kws={'color': 'red'}, size=5.2, aspect=2, ci=99, sharey=True)
```


```python
# 히스토그램에서 적합하는 선은 fit 을 사용함
sns.distplot(Resid_tr_reg1['Error'].iloc[1:], norm_hist='True', fit=stats.norm)
```


```python
# time lag에 따른 잔차 분석
# lag에 상관없이 잔차는 상관관계가 없어야 함
figure, axes = plt.subplots(1, 4, figsize=(30,5))
pd.plotting.lag_plot(Resid_tr_reg1['Error'].iloc[1:], lag=1, ax=axes[0])
pd.plotting.lag_plot(Resid_tr_reg1['Error'].iloc[1:], lag=5, ax=axes[1])
pd.plotting.lag_plot(Resid_tr_reg1['Error'].iloc[1:], lag=10, ax=axes[2])
pd.plotting.lag_plot(Resid_tr_reg1['Error'].iloc[1:], lag=50, ax=axes[3])
```


```python
# 자기상관 분석
# acf 플롯
## pacf 같은 경우는 조금 더 엄격한 기준임
figure, axes = plt.subplots(2,1,figsize=(12,5))
figure = sm.graphics.tsa.plot_acf(Resid_tr_reg1['Error'].iloc[1:], lags=100, use_vlines=True, ax=axes[0])
figure = sm.graphics.tsa.plot_pacf(Resid_tr_reg1['Error'].iloc[1:], lags=100, use_vlines=True, ax=axes[1])
```


```python
# Error Analysis(Statistics)
# 통계값
# Checking Stationarity
# Null Hypothesis: The Time-series is non-stationalry
Stationarity = pd.Series(sm.tsa.stattools.adfuller(Resid_tr_reg1['Error'])[0:4], index=['Test Statistics', 'p-value', 'Used Lag', 'Used Observations'])
for key, value in sm.tsa.stattools.adfuller(Resid_tr_reg1['Error'])[4].items():
    Stationarity['Critical Value(%s)'%key] = value
Stationarity['Maximum Information Criteria'] = sm.tsa.stattools.adfuller(Resid_tr_reg1['Error'])[5]
Stationarity = pd.DataFrame(Stationarity, columns=['Stationarity'])

# Checking of Normality
# Null Hypothesis: The residuals are normally distributed
Normality = pd.DataFrame([stats.shapiro(Resid_tr_reg1['Error'])], index=['Normality'], columns=['Test Statistics', 'p-value']).T

# Checking for Autocorrelation
# Null Hypothesis: Autocorrelation is absent
Autocorrelation = pd.concat([pd.DataFrame(sm.stats.diagnostic.acorr_ljungbox(Resid_tr_reg1['Error'], lags=[1,5,10,50])[0], columns=['Test Statistics']),
                             pd.DataFrame(sm.stats.diagnostic.acorr_ljungbox(Resid_tr_reg1['Error'], lags=[1,5,10,50])[1], columns=['p-value'])], axis=1).T
Autocorrelation.columns = ['Autocorr(lag1)', 'Autocorr(lag5)', 'Autocorr(lag10)', 'Autocorr(lag50)']

# Checking Heteroscedasticity
# Null Hypothesis: Error terms are homoscedastic
Heteroscedasticity = pd.DataFrame([sm.stats.diagnostic.het_goldfeldquandt(Resid_tr_reg1['Error'], X_train.values, alternative='two-sided')],
                                  index=['Heteroscedasticity'], columns=['Test Statistics', 'p-value', 'Alternative']).T
Error_Analysis = pd.concat([Stationarity, Normality, Autocorrelation, Heteroscedasticity], join='outer', axis=1)
Error_Analysis = Error_Analysis.loc[['Test Statistics', 'p-value', 'Alternative', 'Used Lag', 'Used Observations',
                                     'Critical Value(1%)', 'Critical Value(5%)', 'Critical Value(10%)',
                                     'Maximum Information Criteria'],:]
Error_Analysis
```

## Code Summary


```python
### Functionalize
### Error analysis
def stationarity_adf_test(Y_Data, Target_name):
    if len(Target_name) == 0:
        Stationarity_adf = pd.Series(sm.tsa.stattools.adfuller(Y_Data)[0:4],
                                     index=['Test Statistics', 'p-value', 'Used Lag', 'Used Observations'])
        for key, value in sm.tsa.stattools.adfuller(Y_Data)[4].items():
            Stationarity_adf['Critical Value(%s)'%key] = value
            Stationarity_adf['Maximum Information Criteria'] = sm.tsa.stattools.adfuller(Y_Data)[5]
            Stationarity_adf = pd.DataFrame(Stationarity_adf, columns=['Stationarity_adf'])
    else:
        Stationarity_adf = pd.Series(sm.tsa.stattools.adfuller(Y_Data[Target_name])[0:4],
                                     index=['Test Statistics', 'p-value', 'Used Lag', 'Used Observations'])
        for key, value in sm.tsa.stattools.adfuller(Y_Data[Target_name])[4].items():
            Stationarity_adf['Critical Value(%s)'%key] = value
            Stationarity_adf['Maximum Information Criteria'] = sm.tsa.stattools.adfuller(Y_Data[Target_name])[5]
            Stationarity_adf = pd.DataFrame(Stationarity_adf, columns=['Stationarity_adf'])
    return Stationarity_adf

def stationarity_kpss_test(Y_Data, Target_name):
    if len(Target_name) == 0:
        Stationarity_kpss = pd.Series(sm.tsa.stattools.kpss(Y_Data)[0:3],
                                      index=['Test Statistics', 'p-value', 'Used Lag'])
        for key, value in sm.tsa.stattools.kpss(Y_Data)[3].items():
            Stationarity_kpss['Critical Value(%s)'%key] = value
            Stationarity_kpss = pd.DataFrame(Stationarity_kpss, columns=['Stationarity_kpss'])
    else:
        Stationarity_kpss = pd.Series(sm.tsa.stattools.kpss(Y_Data[Target_name])[0:3],
                                      index=['Test Statistics', 'p-value', 'Used Lag'])
        for key, value in sm.tsa.stattools.kpss(Y_Data[Target_name])[3].items():
            Stationarity_kpss['Critical Value(%s)'%key] = value
            Stationarity_kpss = pd.DataFrame(Stationarity_kpss, columns=['Stationarity_kpss'])
    return Stationarity_kpss

def error_analysis(Y_Data, Target_name, X_Data, graph_on=False):
    for x in Target_name:
        Target_name = x
    X_Data = X_Data.loc[Y_Data.index]

    if graph_on == True:
        ##### Error Analysis(Plot)
        Y_Data['RowNum'] = Y_Data.reset_index().index

        # Stationarity(Trend) Analysis
        sns.set(palette="muted", color_codes=True, font_scale=2)
        sns.lmplot(x='RowNum', y=Target_name, data=Y_Data, fit_reg='True', size=5.2, aspect=2, ci=99, sharey=True)
        del Y_Data['RowNum']

        # Normal Distribution Analysis
        figure, axes = plt.subplots(figsize=(12,8))
        sns.distplot(Y_Data[Target_name], norm_hist='True', fit=stats.norm, ax=axes)

        # Lag Analysis
        length = int(len(Y_Data[Target_name])/10)
        figure, axes = plt.subplots(1, 4, figsize=(12,3))
        pd.plotting.lag_plot(Y_Data[Target_name], lag=1, ax=axes[0])
        pd.plotting.lag_plot(Y_Data[Target_name], lag=5, ax=axes[1])
        pd.plotting.lag_plot(Y_Data[Target_name], lag=10, ax=axes[2])
        pd.plotting.lag_plot(Y_Data[Target_name], lag=50, ax=axes[3])

        # Autocorrelation Analysis
        figure, axes = plt.subplots(2,1,figsize=(12,5))
        sm.tsa.graphics.plot_acf(Y_Data[Target_name], lags=100, use_vlines=True, ax=axes[0])
        sm.tsa.graphics.plot_pacf(Y_Data[Target_name], lags=100, use_vlines=True, ax=axes[1])

    ##### Error Analysis(Statistics)
    # Checking Stationarity
    # Null Hypothesis: The Time-series is non-stationalry
    Stationarity_adf = stationarity_adf_test(Y_Data, Target_name)
    Stationarity_kpss = stationarity_kpss_test(Y_Data, Target_name)

    # Checking of Normality
    # Null Hypothesis: The residuals are normally distributed
    Normality = pd.DataFrame([stats.shapiro(Y_Data[Target_name])],
                             index=['Normality'], columns=['Test Statistics', 'p-value']).T

    # Checking for Autocorrelation
    # Null Hypothesis: Autocorrelation is absent
    Autocorrelation = pd.concat([pd.DataFrame(sm.stats.diagnostic.acorr_ljungbox(Y_Data[Target_name], lags=[1,5,10,50])[0], columns=['Test Statistics']),
                                 pd.DataFrame(sm.stats.diagnostic.acorr_ljungbox(Y_Data[Target_name], lags=[1,5,10,50])[1], columns=['p-value'])], axis=1).T
    Autocorrelation.columns = ['Autocorr(lag1)', 'Autocorr(lag5)', 'Autocorr(lag10)', 'Autocorr(lag50)']

    # Checking Heteroscedasticity
    # Null Hypothesis: Error terms are homoscedastic
    Heteroscedasticity = pd.DataFrame([sm.stats.diagnostic.het_goldfeldquandt(Y_Data[Target_name], X_Data.values, alternative='two-sided')],
                                      index=['Heteroscedasticity'], columns=['Test Statistics', 'p-value', 'Alternative']).T
    Score = pd.concat([Stationarity_adf, Stationarity_kpss, Normality, Autocorrelation, Heteroscedasticity], join='outer', axis=1)
    index_new = ['Test Statistics', 'p-value', 'Alternative', 'Used Lag', 'Used Observations',
                 'Critical Value(1%)', 'Critical Value(5%)', 'Critical Value(10%)', 'Maximum Information Criteria']
    Score.reindex(index_new)
    return Score
# error_analysis(Resid_tr_reg1[1:], ['Error'], X_train, graph_on=True)
```

# 기존 Base model의 한계점
- 우리는 미래를 알 수 없다 따라서 그것을 반영한 전처리를 해야함 (data leakage)
- 가장 이상적인 방법은 test set의 시작점부터 하루하루 예측하고 학습시키는 것이 좋음
- 그전에 간단한 방법으로는 시간을 계절성 단위로 잘라서 복사한 후 시계열데이터로 만들고 학습
- 이후에는 스케일링을 진행
    - from sklearn import preprocessing
    - preprocessing.MinMaxScaler()
    - preprocessing.StandardScaler()
    - preprocessing.RobustScaler()
    - preprocessing.Normalizer()
- 마지막으로는 다중공선성 제거
    - 분산팽창지수는 $\frac{1}{1-R^2}$로 풀리는데 
    - 설명변수 하나를 종속변수로 바꾸고 나머지 설명변수로 회귀분석을 구한 R스퀘어를 사용함
    

## 정상성 검정
- ADF-test는 추세를 중점적으로 보는 테스트이기 때문에 추세가 없다면 정상성으로 판단하는 경향이 있음
- KPSS-test는 계절성을 중점적으로 보는 테스트이기 때문에 계절성이 없다면 정상이라고 판단하는 경향이 있음  


```python
# 분산팽창지수는 
def feature_engineering_XbyVIF(X_train, num_variables):
    vif = pd.DataFrame()
    vif['VIF_Factor'] = [variance_inflation_factor(X_train.values, i) 
                         for i in range(X_train.shape[1])]
    vif['Feature'] = X_train.columns
    X_colname_vif = vif.sort_values(by='VIF_Factor', ascending=True)['Feature'][:num_variables].values
    return X_colname_vif
```


```python

```


```python
# 데이터만 추출하기 위한
def industry_analysis_pulse_data(analysis_data,parametric=True,materiality = 'data'):
    #산업군 내 비교를 위한 산업평균 데이터 추가
    avg = analysis_data.groupby('date').mean()
    avg['comp_name'] = '산업평균'
    analysis_data = pd.concat([analysis_data,avg])
    #산업군 내 평균 비교를 위한 순위 생성
    if parametric == False:
        ranking = pd.DataFrame(analysis_data.groupby('comp_name').median()[f'nfr_{materiality}_scaled_pulse']).sort_values(by = f'nfr_{materiality}_scaled_pulse',ascending=False)
    elif parametric == True:
        ranking = pd.DataFrame(analysis_data.groupby('comp_name').mean()[f'nfr_{materiality}_scaled_pulse']).sort_values(by = f'nfr_{materiality}_scaled_pulse',ascending=False)
    ranking['compare_ratio'] = round(100 * (ranking[f'nfr_{materiality}_scaled_pulse'] - ranking.loc['산업평균',f'nfr_{materiality}_scaled_pulse']) / ranking.loc['산업평균',f'nfr_{materiality}_scaled_pulse'],2)
    ranking.drop('산업평균',axis=0,inplace= True)
    ranking['rank'] = ranking[f'nfr_{materiality}_scaled_pulse'].rank(axis = 0,method='max',ascending=False)
    ranking['pct'] = round(100 * ranking['rank'] / ranking.shape[0],2)


##################################  
    # Non-Parametric
    if parametric == False:
        # 산업 평균과 차이가 유의미한지 여부
        result = round(sp.posthoc_conover(analysis_data, val_col = f'nfr_{materiality}_scaled_pulse',group_col = 'comp_name',p_adjust = 'bonferroni'),2)
        
        significance = {}
        part_result = result[result['산업평균'].index != '산업평균']['산업평균'] < 0.05
        for i in range(part_result.shape[0]):
            if part_result[i] == True:
                if ranking.loc[part_result.index[i]].compare_ratio >0:
                    significance[part_result.index[i]] = 1
                elif ranking.loc[part_result.index[i]].compare_ratio <0:
                    significance[part_result.index[i]] = 1
            elif part_result[i] == False:
                significance[part_result.index[i]] = 0
                
        _= pd.DataFrame(significance,index = ['significance']).T
        result = pd.concat([ranking,_],axis=1).sort_values(by='rank')
        return result

    # Parametric        
    elif parametric == True:
        # 산업 평균과 차이가 유의미한지 여부
        hsd = pairwise_tukeyhsd(analysis_data[f'nfr_{materiality}_scaled_pulse'], analysis_data['comp_name'], alpha=0.05)
        results_as_html = hsd.summary().as_html()
        result = pd.read_html(results_as_html, header=0)[0]
        
        significance = {}
        avg_result1 = result[(result.group1 == '산업평균')]
        avg_result2 = result[(result.group2 == '산업평균')]
        avg_result2.columns = ['group2', 'group1', 'meandiff', 'p-adj', 'lower', 'upper', 'reject']
        avg_result = pd.concat([avg_result1,avg_result2]).reset_index(drop=True)
        for i in range(avg_result.shape[0]):
            if avg_result['p-adj'][i] < 0.05:
                if ranking.loc[avg_result.group2[i]].compare_ratio >0:
                    significance[avg_result.group2[i]] = 1
                elif ranking.loc[avg_result.group2[i]].compare_ratio <0:
                    significance[avg_result.group2[i]] = 1
            elif avg_result['p-adj'][i]>=0.05:
                significance[avg_result.group2[i]] = 0
                
        _= pd.DataFrame(significance,index = ['significance']).T
        result = pd.concat([ranking,_],axis=1).sort_values(by='rank')
        return result
```
